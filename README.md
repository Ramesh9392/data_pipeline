# ğŸŒ€ Random User Data Pipeline

A production-ready Apache Airflow pipeline that fetches random user data from an external API, processes and loads it into a PostgreSQL database, and sends a formatted email summary report. This DAG includes robust features such as data quality checks, conditional branching, audit logging, and notification management.

---

## ğŸš€ Features

- âœ… **Scheduled Ingestion**: Runs daily to pull random user data from an external API.
- ğŸ” **Branching Logic**: Smart branching to skip steps if data already exists.
- ğŸ—ƒï¸ **PostgreSQL Integration**: Loads data into a relational database using parameterized queries.
- ğŸ§ª **Data Quality Check**: Uses SQL sensor to ensure data freshness before ingestion.
- ğŸ“§ **Email Notifications**: Sends an HTML summary email after each DAG run.
- ğŸ“œ **Audit Logging**: Logs ingestion metadata to a dedicated audit table.
- ğŸ§± **Modular Design**: Clean project structure for easy extension and testing.
- ğŸ³ **Dockerized Environment**: Airflow + PostgreSQL run in containers.
- âš™ï¸ **Environment Configurable**: Easy setup using a `.env` file.
- ğŸ§  **Conditional Inserts**: Data inserted only if daily conditions are met.

---

## ğŸ“ Project Structure
.
â”œâ”€â”€ dags/
â”‚ â””â”€â”€ random_user_pipeline.py # Main DAG definition
â”œâ”€â”€ ingestion/
â”‚ â””â”€â”€ api_fetcher/
â”‚ â””â”€â”€ fetch_random_user.py # Fetches data from external API
â”œâ”€â”€ storage/
â”‚ â””â”€â”€ postgres_loader/
â”‚ â””â”€â”€ write_to_postgres.py # Loads data into PostgreSQL
â”œâ”€â”€ utils/
â”‚ â”œâ”€â”€ branch_decision.py # Decides whether to fetch data
â”‚ â”œâ”€â”€ skip_task_xcoms.py # Handles XComs for skipped paths
â”‚ â””â”€â”€ email_content_template.html # HTML template for summary email
â”œâ”€â”€ sql/
â”‚ â”œâ”€â”€ check_fresh_data.sql # SQL sensor query
â”‚ â””â”€â”€ log_ingestion.sql # SQL audit logging
â”œâ”€â”€ docker-compose.yml # Docker config for Airflow + Postgres
â”œâ”€â”€ .env.example # Template for environment variables
â”œâ”€â”€ requirements.txt # Optional Python dependencies
â””â”€â”€ README.md # This file


---

## ğŸ› ï¸ Technologies Used

- **[Apache Airflow](https://airflow.apache.org/)** â€“ Workflow orchestration
- **Python 3.8+** â€“ Scripting and pipeline logic
- **PostgreSQL** â€“ Data storage and audit logging
- **SQL** â€“ Sensors and inserts
- **Jinja2/HTML** â€“ Email templating
- **XComs & Trigger Rules** â€“ Task flow control
- **Docker Compose** â€“ Local deployment of Airflow & Postgres

---

## ğŸ§ª Trigger Rules

Airflow DAG uses conditional task execution:
- Skip steps if data already exists for the current date
- Insert data only if at least 1 record exists and fewer than 150 records are already present

---

## ğŸ“¬ Example Email Report

The email contains:
- Number of records ingested
- Timestamp of ingestion
- DAG execution status  
All presented in a clean, structured HTML template.

---

## ğŸ§° Setup Instructions

### 1ï¸âƒ£ Clone the Repository

##  Start the Pipeline Locally
```bash
git clone https://github.com/Ramesh9392/data_pipeline.git
cd data_pipeline
```
Configure Environment Variables
Copy the example config file:

```bash
cp .env_example .env

```
Edit .env and update your local values:
```bash
# Airflow Admin Login
AIRFLOW_ADMIN_USERNAME=admin
AIRFLOW_ADMIN_PASSWORD=admin123

# PostgreSQL Config
POSTGRES_USER=airflow
POSTGRES_PASSWORD=airflow
POSTGRES_DB=airflow
POSTGRES_HOST=postgres
POSTGRES_PORT=5432

# SMTP Email Settings
smtp_server=smtp.example.com
smtp_port=587
smtp_username=your_username
smtp_password=your_password
smtp_sender_email=airflow@example.com
```

## 3ï¸âƒ£ Start the Pipeline Locally


```bash
go to project by using cd and in project folder run the following command:

docker compose up --build
```
This will:

Start Airflow and Postgres services

Initialize the metadata DB

Create an admin user

Launch the webserver and scheduler

## ğŸŒ Access the Airflow UI:

http://localhost:8080

## 4ï¸âƒ£ Stop the Pipeline
```bash
docker compose down
```
To also remove DB volumes and logs:
```bash
docker compose down -v
```